{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics and machine learning, **regularization** is used to help avoid the problem of overfitting.\n",
    "\n",
    "Models that are overfit can effectively explain observed data but generalize poorly to unseen data. \n",
    "\n",
    "Regularization stops our model overfitting our training data by constraining it's learned parameters, this in turn stops any variable/s having too much influence on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the least squares solution to the linear regression problem $w_{ls}$ given a set of observations $y$ and data $X$ is equal to $(X^TX)^{-1}X^Ty$. \n",
    "\n",
    "Least squares also has a probabilistic interpretation which allows us to understand some interesting properties about our feature vector $w$.\n",
    "\n",
    "If we imagine our observations are drawn from a normal distribution with $\\mu = Xw$ and a diagonal covariance matrix $\\Sigma = \\sigma^2I$ we can then solve for $w$ using maximum likelihood estimation.\n",
    "\n",
    "$$ w_{ML} = arg\\,max\\;\\;ln(p(y|\\mu = Xw))$$\n",
    "\n",
    "$$ w_{ML} = -\\frac{1}{2\\sigma^{-2}}\\,\\Vert y - Xw \\Vert^2 - \\frac{n}{2} ln(2\\pi\\sigma^2)$$\n",
    "\n",
    "Least squares (LS) and maximum likelihood (ML) share the same solution:\n",
    "\n",
    "$$ LS:   arg min\\; \\Vert y - Xw\\Vert^2\\;\\Leftarrow\\Rightarrow\\;ML:   arg max\\; -\\frac{1}{2\\sigma^{-2}}\\,\\Vert y - Xw \\Vert^2 $$\n",
    "\n",
    "In a sense we are making an independent Gaussian noise assumption about the error term $\\epsilon$, this is equivalent to $$y_i = x_i^Tw + \\epsilon_i, \\epsilon_i \\sim N(0,\\sigma^2) \\equiv y \\sim N(Xw, \\sigma^2)$$\n",
    "\n",
    "Under the gaussian assumption $y \\sim N(Xw,\\sigma^2)$ it can be shown that $$ \\mathbb E[w_{ml}]= w$$ and $$Var[w_{ml}]=\\sigma^2(X^TX)^{-1}$$\n",
    "\n",
    "This implies that the maximum likelihood estimate of w is unbiased, however when the value $\\sigma^2(X^TX)^{-1}$ is very large then our derived $w$ is extremely sensitive to the observed $y$\n",
    "\n",
    "One way of constraining the the values in $w$ is to apply a penalty term $\\lambda g(w)$ to the least squares objective function so that it becomes: $$w = argmin \\; \\Vert y - Xw \\Vert + \\lambda g(w)$$ where $lambda > 0 :$ is a regularisation parameter and $g(w) > 0$ is a function that enforces certain desrired characters about w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "Ridge regression is one such regularization method — it uses the squared penalty on the regularisation parameter \n",
    "\n",
    "$$w_{rr} = argmin \\; \\Vert y - Xw \\Vert + \\lambda \\Vert w \\Vert^2$$.\n",
    "\n",
    "The inclusion of $\\Vert w \\Vert^2$ as the regularisation parameter penalises large values in $w$.\n",
    "\n",
    "\n",
    "We can solve for the ridge regression solution in exactly the same manor as we do for ordinary least squares.\n",
    "\n",
    "$$ L = \\Vert y - XW \\Vert ^2 + \\lambda \\Vert w \\Vert^2 = (y - Xw)^T(y-Xw)+\\lambda w^Tw $$\n",
    "\n",
    "To solve we take the gradient of $L$ w.r.t $w$ and set to zero\n",
    "\n",
    "$$\\nabla L = 2X^Ty+2X^TXw+2\\lambda w = 0$$\n",
    "\n",
    "it then follows that:\n",
    "\n",
    "$$ w_{rr} = (\\lambda I + X^TX)^{-1}X^Ty $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Code Example\n",
    "\n",
    "Let's take a look at the House Prices dataset from Kaggle. The data contains 79 explanatory variables which describe properties of houses in Ames, Iowa as well there selling price. \n",
    "\n",
    "GIven the available data let's try to build a model that can predict the selling price of a house given a set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1460 entries, 1 to 1460\n",
      "Data columns (total 80 columns):\n",
      "MSSubClass       1460 non-null int64\n",
      "MSZoning         1460 non-null object\n",
      "LotFrontage      1201 non-null float64\n",
      "LotArea          1460 non-null int64\n",
      "Street           1460 non-null object\n",
      "Alley            91 non-null object\n",
      "LotShape         1460 non-null object\n",
      "LandContour      1460 non-null object\n",
      "Utilities        1460 non-null object\n",
      "LotConfig        1460 non-null object\n",
      "LandSlope        1460 non-null object\n",
      "Neighborhood     1460 non-null object\n",
      "Condition1       1460 non-null object\n",
      "Condition2       1460 non-null object\n",
      "BldgType         1460 non-null object\n",
      "HouseStyle       1460 non-null object\n",
      "OverallQual      1460 non-null int64\n",
      "OverallCond      1460 non-null int64\n",
      "YearBuilt        1460 non-null int64\n",
      "YearRemodAdd     1460 non-null int64\n",
      "RoofStyle        1460 non-null object\n",
      "RoofMatl         1460 non-null object\n",
      "Exterior1st      1460 non-null object\n",
      "Exterior2nd      1460 non-null object\n",
      "MasVnrType       1452 non-null object\n",
      "MasVnrArea       1452 non-null float64\n",
      "ExterQual        1460 non-null object\n",
      "ExterCond        1460 non-null object\n",
      "Foundation       1460 non-null object\n",
      "BsmtQual         1423 non-null object\n",
      "BsmtCond         1423 non-null object\n",
      "BsmtExposure     1422 non-null object\n",
      "BsmtFinType1     1423 non-null object\n",
      "BsmtFinSF1       1460 non-null int64\n",
      "BsmtFinType2     1422 non-null object\n",
      "BsmtFinSF2       1460 non-null int64\n",
      "BsmtUnfSF        1460 non-null int64\n",
      "TotalBsmtSF      1460 non-null int64\n",
      "Heating          1460 non-null object\n",
      "HeatingQC        1460 non-null object\n",
      "CentralAir       1460 non-null object\n",
      "Electrical       1459 non-null object\n",
      "1stFlrSF         1460 non-null int64\n",
      "2ndFlrSF         1460 non-null int64\n",
      "LowQualFinSF     1460 non-null int64\n",
      "GrLivArea        1460 non-null int64\n",
      "BsmtFullBath     1460 non-null int64\n",
      "BsmtHalfBath     1460 non-null int64\n",
      "FullBath         1460 non-null int64\n",
      "HalfBath         1460 non-null int64\n",
      "BedroomAbvGr     1460 non-null int64\n",
      "KitchenAbvGr     1460 non-null int64\n",
      "KitchenQual      1460 non-null object\n",
      "TotRmsAbvGrd     1460 non-null int64\n",
      "Functional       1460 non-null object\n",
      "Fireplaces       1460 non-null int64\n",
      "FireplaceQu      770 non-null object\n",
      "GarageType       1379 non-null object\n",
      "GarageYrBlt      1379 non-null float64\n",
      "GarageFinish     1379 non-null object\n",
      "GarageCars       1460 non-null int64\n",
      "GarageArea       1460 non-null int64\n",
      "GarageQual       1379 non-null object\n",
      "GarageCond       1379 non-null object\n",
      "PavedDrive       1460 non-null object\n",
      "WoodDeckSF       1460 non-null int64\n",
      "OpenPorchSF      1460 non-null int64\n",
      "EnclosedPorch    1460 non-null int64\n",
      "3SsnPorch        1460 non-null int64\n",
      "ScreenPorch      1460 non-null int64\n",
      "PoolArea         1460 non-null int64\n",
      "PoolQC           7 non-null object\n",
      "Fence            281 non-null object\n",
      "MiscFeature      54 non-null object\n",
      "MiscVal          1460 non-null int64\n",
      "MoSold           1460 non-null int64\n",
      "YrSold           1460 non-null int64\n",
      "SaleType         1460 non-null object\n",
      "SaleCondition    1460 non-null object\n",
      "SalePrice        1460 non-null int64\n",
      "dtypes: float64(3), int64(34), object(43)\n",
      "memory usage: 923.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"housing-data.csv\",index_col=0)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [\"PoolQC\",\"Fence\",\"MiscFeature\",\"FireplaceQu\",\"Alley\"]\n",
    "\n",
    "df = df[[c for c in df.columns if c not in cols_to_drop]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n",
       "Id                                                                          \n",
       "1           60       RL         65.0     8450   Pave      Reg         Lvl   \n",
       "2           20       RL         80.0     9600   Pave      Reg         Lvl   \n",
       "3           60       RL         68.0    11250   Pave      IR1         Lvl   \n",
       "4           70       RL         60.0     9550   Pave      IR1         Lvl   \n",
       "5           60       RL         84.0    14260   Pave      IR1         Lvl   \n",
       "\n",
       "   Utilities LotConfig LandSlope    ...    EnclosedPorch 3SsnPorch  \\\n",
       "Id                                  ...                              \n",
       "1     AllPub    Inside       Gtl    ...                0         0   \n",
       "2     AllPub       FR2       Gtl    ...                0         0   \n",
       "3     AllPub    Inside       Gtl    ...                0         0   \n",
       "4     AllPub    Corner       Gtl    ...              272         0   \n",
       "5     AllPub       FR2       Gtl    ...                0         0   \n",
       "\n",
       "   ScreenPorch PoolArea MiscVal  MoSold  YrSold  SaleType  SaleCondition  \\\n",
       "Id                                                                         \n",
       "1            0        0       0       2    2008        WD         Normal   \n",
       "2            0        0       0       5    2007        WD         Normal   \n",
       "3            0        0       0       9    2008        WD         Normal   \n",
       "4            0        0       0       2    2006        WD        Abnorml   \n",
       "5            0        0       0      12    2008        WD         Normal   \n",
       "\n",
       "   SalePrice  \n",
       "Id            \n",
       "1     208500  \n",
       "2     181500  \n",
       "3     223500  \n",
       "4     140000  \n",
       "5     250000  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df,drop_first=True)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "\n",
    "X_train,X_test = df.drop(\"SalePrice\", axis=1)[msk].as_matrix(), df.drop(\"SalePrice\", axis=1)[~msk].as_matrix()\n",
    "\n",
    "y_train, y_test = df[\"SalePrice\"][msk].as_matrix(), df[\"SalePrice\"][~msk].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As ridge regression imposes a penalty on the size of regression coefficients, we need a way of making sure that our\n",
    "coefficients are on the same scale so that the penalties are applied fairly. \n",
    "\n",
    "To do this we make sure our input data has mean zero and unit variance (z-scoring), we also centre our dependent variable $y$ so that it has mean zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def standardize_matrix(X):\n",
    "    \n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    return X_std\n",
    "\n",
    "X_train,X_test = standardize_matrix(X_train),standardize_matrix(X_test)\n",
    "\n",
    "y_train = y_train - y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a ridge regression class. We can then use it to fit a model to our training data and in turn estimate a feature vector $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25346.931006886953"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RidgeRegression():\n",
    "    \n",
    "    #Fits a model to our training data and in turn produces a feature vector w\n",
    "    def fit(self, X, y, l=1):\n",
    "        \n",
    "        #Number of features in our dataset\n",
    "        p = X.shape[1]\n",
    "        \n",
    "        #Transpose our feature matrix X\n",
    "        X_T = X.T\n",
    "        \n",
    "        #Identity matrix of size p\n",
    "        eye = np.eye(p)\n",
    "        \n",
    "        C = l * eye + X.T.dot(X)\n",
    "        \n",
    "        # w = (lambda*I + X^tX)^-1 X^tY\n",
    "        self.w = np.linalg.inv(C).dot(X.T.dot(y))\n",
    "    \n",
    "    #Predicts dependent variable y for a matrix with the same number of columns as the training data\n",
    "    def predict(self, X):\n",
    "        \n",
    "        return X.dot(self.w)\n",
    "\n",
    "r = RidgeRegression()\n",
    "\n",
    "r.fit(X_train, y_train,l=100)\n",
    "\n",
    "y_pred = r.predict(X_train)\n",
    "\n",
    "np.sqrt(np.square(y_pred - y_train).sum() / len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21056.526384345994"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = RidgeRegression()\n",
    "\n",
    "r.fit(X_train, y_train,l=0.01)\n",
    "\n",
    "y_pred = r.predict(X_train)\n",
    "\n",
    "np.sqrt(np.square(y_pred - y_train).sum() / len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21056.526384346002"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "clf = Ridge(alpha=0.01)\n",
    "\n",
    "clf.fit(X_train, y_train) \n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "\n",
    "np.sqrt(np.square(y_pred - y_train).sum() / len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21056.566401793225"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "clf = LinearRegression()\n",
    "\n",
    "clf.fit(X_train, y_train) \n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "\n",
    "np.sqrt(np.square(y_pred - y_train).sum() / len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21057.42025628706"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "class LassoRegression():\n",
    "    \n",
    "\n",
    "    def fit(self, X, y, lam=1):\n",
    "        \n",
    "        p = X.shape[1]\n",
    "        \n",
    "        w0 = np.random.rand(p)\n",
    "        \n",
    "        func = lambda w,y,x,lam: ((y - x.dot(w))**2).sum() + lam*w.sum()\n",
    "        \n",
    "        self.w = minimize(func, w0, args=(y,X,lam))[\"x\"]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        return X.dot(self.w)\n",
    "        \n",
    "\"\"\"    \n",
    "    return ((y - X.dot(w))**2).sum() + lam*w.sum()\n",
    "\n",
    "\n",
    "W = minimize(func, np.random.rand(X_train.shape[1]), args=(y_train, X_train, 100))\n",
    "\"\"\"\n",
    "\n",
    "clf = LassoRegression()\n",
    "clf.fit(X_train, y_train,0.1) \n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "\n",
    "np.sqrt(np.square(y_pred - y_train).sum() / len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "func = lambda w,y,x: ((y - x.dot(w))**2).sum() + 1.0*w.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(np.array([1]),np.array([1]),np.array([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21062.029348836368"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.square(y_train - X_train.dot(W)).sum() / len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = np.random.rand(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "755321.53245057422"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(y_train - X_train.dot(r.w),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(888,)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(888, 221)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3460.9763339317606"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_train - X_train.dot(r.w)).sum() + w.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25730.13851952718"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RidgeRegressor(object):\n",
    "    \"\"\"\n",
    "    Linear Least Squares Regression with Tikhonov regularization.\n",
    "    More simply called Ridge Regression.\n",
    "    We wish to fit our model so both the least squares residuals and L2 norm\n",
    "    of the parameters are minimized.\n",
    "    argmin Theta ||X*Theta - y||^2 + alpha * ||Theta||^2\n",
    "    A closed form solution is available.\n",
    "    Theta = (X'X + G'G)^-1 X'y\n",
    "    Where X contains the independent variables, y the dependent variable and G\n",
    "    is matrix alpha * I, where alpha is called the regularization parameter.\n",
    "    When alpha=0 the regression is equivalent to ordinary least squares.\n",
    "    http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)\n",
    "    http://en.wikipedia.org/wiki/Tikhonov_regularization\n",
    "    http://en.wikipedia.org/wiki/Ordinary_least_squares\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y, alpha=0):\n",
    "        \"\"\"\n",
    "        Fits our model to our training data.\n",
    "        Arguments\n",
    "        ----------\n",
    "        X: mxn matrix of m examples with n independent variables\n",
    "        y: dependent variable vector for m examples\n",
    "        alpha: regularization parameter. A value of 0 will model using the\n",
    "        ordinary least squares regression.\n",
    "        \"\"\"\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        G = alpha * np.eye(X.shape[1])\n",
    "        G[0, 0] = 0  # Don't regularize bias\n",
    "        self.params = np.dot(np.linalg.inv(np.dot(X.T, X) + np.dot(G.T, G)),\n",
    "                             np.dot(X.T, y))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the dependent variable of new data using the model.\n",
    "        The assumption here is that the new data is iid to the training data.\n",
    "        Arguments\n",
    "        ----------\n",
    "        X: mxn matrix of m examples with n independent variables\n",
    "        alpha: regularization parameter. Default of 0.\n",
    "        Returns\n",
    "        ----------\n",
    "        Dependent variable vector for m examples\n",
    "        \"\"\"\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        return np.dot(X, self.params)\n",
    "\n",
    "r = RidgeRegressor()\n",
    "\n",
    "r.fit(X_train,y_train,10)\n",
    "\n",
    "y_pred = r.predict(X_train)\n",
    "\n",
    "np.sqrt(np.square(y_pred - y_train).sum() / len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 200329.04224737,  205025.7866696 ,  201719.81815852,\n",
       "        162748.1825252 ,  294229.74646662,  119512.03361856,\n",
       "        284563.50232795,  132922.08983185,  114623.6370127 ,\n",
       "        392210.09571446])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  13531.44384922,   18228.18827142,   14922.21976037,\n",
       "        -24049.41587295,  107432.14806847,  -67285.56477961,\n",
       "         97765.90392975,  -53875.50856632,  -72173.96138548,\n",
       "        205412.49731629])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dot(w)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09866795, -0.24877704, -0.20127756, ..., -0.13055824,\n",
       "         0.49300665, -0.34662687],\n",
       "       [-0.85472276,  0.34150783, -0.06727982, ..., -0.13055824,\n",
       "         0.49300665, -0.34662687],\n",
       "       [ 0.09866795, -0.13072007,  0.1249778 , ..., -0.13055824,\n",
       "         0.49300665, -0.34662687],\n",
       "       ..., \n",
       "       [ 0.33701563, -0.20942472, -0.13229785, ..., -0.13055824,\n",
       "         0.49300665, -0.34662687],\n",
       "       [-0.85472276, -0.13072007, -0.053647  , ..., -0.13055824,\n",
       "         0.49300665, -0.34662687],\n",
       "       [-0.85472276,  0.1447462 , -0.02801265, ..., -0.13055824,\n",
       "         0.49300665, -0.34662687]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
